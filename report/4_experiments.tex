\section{Experiments}
In this section we report on the experimental evaluation and comparison of the click models in Section~\ref{sec:methodology} on the evaluation metrics in Section~\ref{sec:evaluation} evaluated on the first 1 million query sessions from the Yandex Relevance Prediction contest of 2011 \ref{YandexData}. The rest of this section will elaborate on the experiments we have done and the results we found. We will also show how query frequency improves the performance of the models. Moreover, we will report on the evaluation measures in combination with click entropy.

\subsection{Experimental Setup}
The Yandex data set was used in the Yandex Relevance Prediction contest of 2011. In the experiments we have done we have used the first 1 million queries. We deliberatly have chosen to keep sessions that are without any clicks because removing them might hurt the performance of certain models, i.e. TCM because the freshness of a document can change with these sessions. These first 1 million sessions contain 450,931 distinct queries. These queries get divided into a set of training sessions used to train the click models and a set of test sessions used in the evaluation of the models, the number of sessions in these sets have a 3 to 1 ratio. The problem with this approach in combination with the TCM model is that this removes the guarantee that the complete task is used when training or evaluating the model. We expect that this will hurt the performance of the TCM model and that the actual performance might be higher. 

To report on the performance with regard to query frequency we have split the data into 4 parts. The distribution of these parts can be seen in Table~\ref{table:query_split}.We see that there are a lot of queries that are only seen once, these queries are not informative for the models that only look at relevance, e.g. the CTR and CM models, however they are informative for the other models to infer other parameters than relevance. We also see that there are few queries In the rest of this section with every evaluation measure we will also report on how this was influenced by the query frequency. The second factor that might influence performance that we have analyzed is the click entropy. The click entropy was also used to analyse queries in \cite{dou2008}. The formal definition is shown in Equation~\ref{eq:entropy}.

\begin{table}
\centering
\begin{tabular}{|l|l|}
\hline
Query frequency & Number of sessions \\ \hline
1               & 392508            \\ \hline
2 - 5           & 123799            \\ \hline
6 - 19          & 63596             \\ \hline
20 +            & 420097            \\ \hline
\end{tabular}
\caption{The distribution of session with respect to query frequency}
\label{table:query_split}
\end{table}

\begin{align}
	ClickEntropy(q) &= \sum_{d \in \mathcal{P}(q)} - P(d|q) \log_2 P(d|q) \label{eq:entropy} \\
	P(d|q) &= \frac{\sum_p c_{r_d}^{(q)}}{\sum_{u \in \mathcal{P}(q)} c_{r_u}^{(q)}} \label{eq:entropy_2}
\end{align}

Here $ClickEntropy(q)$ is the click entropy for query $q$. $\mathcal{P}(q)$ are documents clicked on when regarding query $q$. $P(d|q)$ is the percentage of clicks on document $d$ among all clicks on $q$. The search sessions have been divided into 3 parts with respect to click entropy and the evaluation measures have been tested on these parts. Statistics of these parts can be seen in Table~\ref{table:entropy_split}.

\begin{table}
\centering
\begin{tabular}{|l|l|}
\hline
Click entropy & Number of sessions \\ \hline
0 - 1         & 509062             \\ \hline
1+ - 2        & 154672             \\ \hline
2+            & 336266             \\ \hline
\end{tabular}
\caption{The distribution of session with respect to click entropy}
\end{table}

\subsection{Results}
In this section we outline the results of the experiments. For every evaluation measure the influence of the query frequency and click entropy can be seen. Table~\ref{table:results} contains the values of the evaluation measures for every model when trained on the entire dataset.

\subsubsection{Results on Loglikelihood}
In Figures \ref{fig:ll_qf} and \ref{fig:ll_ce} the results of the loglikelihood experiments can be seen.

\subsubsection{Results on Perplexity}

\subsubsection{Results on CTR-Prediction}

\subsubsection{Results on Relevance Prediction}

\subsubsection{Results Predicted Relevance as a Ranking Feature}

\subsubsection{Results on Computation Time}

In Table~\ref{table:results} one can see the results of the experiments.

\begin{table}[h]
	\begin{tabular}{@{}lllllll@{}}
		\toprule
		Model     & Log-likelihood  & Perplexity	 & Rel. Pred. AUC		 & Ranking NDCG      & CTR Pred.		 & Training time (sec) \\ \midrule
		UBM       & \textbf{-0.288} & 1.382          & 0.699                 & 0.875			 & 0.218			 & 10387.7                 \\
		TCM       & -0.307          & \textbf{1.374} & 0.532                 & \textbf{0.977}	 & 0.234			 & 8145.77                 \\
		SimpleDCM & -0.450          & 1.387          & 0.674                 & 0.837             & \textbf{0.210}	 & 186.915                 \\
		SimpleDBN & -0.448          & 1.386          & 0.668                 & 0.740             & \textbf{0.210}	 & \textbf{161.067}        \\
		DCM       & -0.453          & 1.380          & \textbf{0.706}        & 0.825			 & 0.228			 & 18332.5                 \\
		DBN       & -0.375          & 1.383          & 0.436                 & 0.681			 & 0.230			 & 13204.2                 \\ \bottomrule
	\end{tabular}
	\caption{Results}
	\label{table:results}
\end{table}
