\section{Experiments}
In this section we report on the experimental evaluation and comparison of the click models in Section~\ref{sec:methodology} on the evaluation metrics in Section~\ref{sec:evaluation} evaluated on the first 1 million query sessions from the Yandex Relevance Prediction contest of 2011 \ref{YandexData}. The rest of this section will elaborate on the experiments we have done and the results we found. We will also show how query frequency improves the performance of the models. Moreover, we will report on the evaluation measures in combination with click entropy.

\subsection{Experimental Setup}
The Yandex data set was used in the Yandex Relevance Prediction contest of 2011. In the experiments we have done we have used the first 1 million queries. We deliberatly have chosen to keep sessions that are without any clicks because removing them might hurt the performance of certain models, i.e. TCM because the freshness of a document can change with these sessions. These first 1 million sessions contain 450,931 distinct queries. These queries get divided into a set of training sessions used to train the click models and a set of test sessions used in the evaluation of the models, the number of sessions in these sets have a 3 to 1 ratio. The problem with this approach in combination with the TCM model is that this removes the guarantee that the complete task is used when training or evaluating the model. We expect that this will hurt the performance of the TCM model and that the actual performance might be higher. 

\subsubsection{Performance impacting factors}
To report on the performance with regard to query frequency we have split the data into 4 parts. The distribution of these parts can be seen in Table~\ref{table:query_split}.We see that there are a lot of queries that are only seen once, these queries are not informative for the models that only look at relevance, e.g. the CTR and CM models, however they are informative for the other models to infer other parameters than relevance. We also see that there are few queries In the rest of this section with every evaluation measure we will also report on how this was influenced by the query frequency. We did this by only using the sessions for a particular group of query frequencies. This way all parameters can be optimal for these frequencies. This does raise a problem however for the query frequency bin of 6 to 19, here there are so few queries it is hard for the models to learn the correct parameters. This will have a negative effect on the performance of the models. 

The second factor that might influence performance that we have analyzed is the click entropy. The click entropy was also used to analyse queries in \cite{dou2008}.The formal definition is shown in Equation~\ref{eq:entropy}.

\begin{table}
\centering
\begin{tabular}{|l|l|}
\hline
Query frequency & Number of sessions \\ \hline
1               & 392508            \\ \hline
2 - 5           & 123799            \\ \hline
6 - 19          & 63596             \\ \hline
20 +            & 420097            \\ \hline
\end{tabular}
\caption{The distribution of session with respect to query frequency}
\label{table:query_split}
\end{table}

\begin{align}
	ClickEntropy(q) &= \sum_{d \in \mathcal{P}(q)} - P(d|q) \log_2 P(d|q) \label{eq:entropy} \\
	P(d|q) &= \frac{\sum_p c_{r_d}^{(q)}}{\sum_{u \in \mathcal{P}(q)} c_{r_u}^{(q)}} \label{eq:entropy_2}
\end{align}

Here $ClickEntropy(q)$ is the click entropy for query $q$. $\mathcal{P}(q)$ are documents clicked on when regarding query $q$. $P(d|q)$ is the percentage of clicks on document $d$ among all clicks on $q$.  Click entropy can be used to split the data into informational and navigational queries. In navigational queries users know what they are looking for so the click entropy will be low because almost all clicks within that query will be on the same document. In a informational query the users explore different results to find the optimal one because they don't know what document they are looking for yet. This will give these queries a high click entropy.
The search sessions have been divided into 3 parts with respect to click entropy and the evaluation measures have been tested on these parts in the same way as query frequency. Statistics of these parts can be seen in Table~\ref{table:entropy_split}.

\begin{table}
\centering
\begin{tabular}{|l|l|}
\hline
Click entropy & Number of sessions \\ \hline
0 - 1         & 509062             \\ \hline
1+ - 2        & 154672             \\ \hline
2+            & 336266             \\ \hline
\end{tabular}
\caption{The distribution of session with respect to click entropy}
\end{table}

\subsection{Results}
In this section we outline the results of the experiments. For every evaluation measure the influence of the query frequency and click entropy can be seen. Table~\ref{table:results} contains the values of the evaluation measures for every model when trained on the entire dataset.

\subsubsection{Loglikelihood}
In Figures \ref{fig:ll_qf} and \ref{fig:ll_ce} the results of the loglikelihood experiments can be seen.  The Loglikelihood of the Cascade Model is not measured as the likelihood of a session with multiple clicks is undefined in that model. We see that UBM is the model that performs best when training on the whole data set and only on the queries with average click-entropy, between 1 and 2, the model is outperformed by CCM. We expected UBM to be the best model going into these experiments. Something to note is that the PBM performs well on queries that have low entropy, a sign that the examination parameters are more accurate on informational queries.

Query frequency has \fixme{Fill in something about the ll\_qf graph.}

\subsubsection{Perplexity}
In Figures~\ref{fig:perp_qf} and \ref{fig:perp_ce} the influence of the query frequency and click entropy on the perplexity can be seen. 

In Figure~\ref{fig:perp_rank} the perplexity of different ranks can be seen. As a document has a higher rank the models perplexity gets lower. However speed with which it gets lower differs per model. TCM and PBM have a high perplexity for the lower ranks, the documents that are more often examined by the users. Whereas for high ranks they are the top performers in terms of perplexity. What this shows is that position based click models, of which our implementation of TCM is one, the examination parameter is more accurate than the ones that are inferred by other models, e.g. UBM. 

\subsubsection{CTR-Prediction}
Figures~\ref{fig:ctr_qf} and \ref{fig:ctr_ce} show the impact of query frequency and click entropy on the CTR-prediction task. In this task the simple models, CTR and CM, outperform the more complex ones. This is because the intuition of these models is exactly what this task has set out to measure. As seen in the explanation of this task in Section~\ref{sec:ctr} this task is designed to take away the position bias by only testing on the sessions where the document appears in the first place. However it is dissappointing that the complex models perform so much worse. This may have to do with the fact that every time the models are trained on a very small sample of the entire dataset. A consequence of this is that the other parameters, that are used by the complex models besides the relevance parameter, have little data to train on which influences the relevance parameter. Because of this we propose to change the method for calculating the CTR-prediction. By not training on one query at a time but on all queries in the same time the other parameters will train better and thus models that use EM-inference have a better chance to learn the actual relevance of a document.

\subsubsection{Relevance Prediction}
The results of the relevance prediction task can be seen in Figures~\ref{fig:rel_qf} and~\ref{fig:rel_ce}. Because of the way that the relevance prediction is calculated the results for the different splits of data are not as interesting. The models are trained only on the small subset of data and are evaluated on all query-document pairs found in the set of annotated relevances a lot of pairs are never seen before by the model. The model will then predict $0.5$, whereas the annotated relevance is always $0$ or $1$. This means that for all unseen query-document pairs the model will get a RMSE of 0.5 and the effect of document-query pairs that are seen will be negligible. When looking at the models that are trained on all the data we can see that the AUC gets lower then 0.5 this means that the predicted relevances have a negative correlation with the annotated, which is illustrated by the Pearson correlation values we found. The values that were found there were between the 0.05 and -0.05, depending on whether the AUC was above or below 0.5. The p-values that we have found were all below 0.05 and indicate that the test is statiscally significant.

\subsubsection{Predicted Relevance as a Ranking Feature}
In Figures~\ref{fig:rank_qf} and~\ref{fig:rank_ce} the results of using the predicted relevance as a ranking feature can be seen.

\subsubsection{Computation time}
In Table~\ref{table:results} we see that, as expected, the models that use MLE-inference are much faster than the models with EM-inference. These training times depend on the number of queries that the model is trained upon and the amount of parameters a model has. An interesting thing to note is that the UBM model out performs TCM and CCM by a large margin.



\begin{table}[h]
	\centering
	\begin{tabular}{@{}lcccccc@{}}
		\toprule
		Model     & Log-likelihood  & Perplexity	 & Rel. Pred. AUC		 & Ranking NDCG      & CTR Pred.		 & Training time (sec) \\ \midrule
		UBM       & \textbf{-0.288} & 1.382          & 0.699                 & 0.875			 & 0.218			 & 10387.7                 \\
		TCM       & -0.307          & \textbf{1.374} & 0.532                 & \textbf{0.977}	 & 0.234			 & 8145.77                 \\
		SimpleDCM & -0.450          & 1.387          & 0.674                 & 0.837             & \textbf{0.210}	 & 186.915                 \\
		SimpleDBN & -0.448          & 1.386          & 0.668                 & 0.740             & \textbf{0.210}	 & \textbf{161.067}        \\
		DCM       & -0.453          & 1.380          & \textbf{0.706}        & 0.825			 & 0.228			 & 18332.5                 \\
		DBN       & -0.375          & 1.383          & 0.436                 & 0.681			 & 0.230			 & 13204.2                 \\ \bottomrule
	\end{tabular}
	\caption{Results}
	\label{table:results}
\end{table}
