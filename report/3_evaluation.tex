\section{Evaluation Measures}
\label{sec:evaluation}
%Evaluation:
%	Purpose of the evaluation
%	Data used in the evaluation
%	Evaluation setup
%	Results (present as many results as necessary to illustrate the work you have done on the project - tables are a good way and required by the assignment; plots can be another good way)
To equally evaluate each click model's performance, we use evaluation metrics that are proposed along with the click models. The evaluation metrics used in this experiment are listed below:

\subsection{Loglikelihood}
1Loglikelihood is the default evaluation metric in machine learning. It says something about the likelihood of the data give the model. In Equation~\ref{eq:loglikelihood} the calculation of the loglikelihood of a click model and a set of sessions can be seen, where $\mathcal{L}\mathcal{L}(S|\mathbf{M})$ is the loglikelihood of the sessions given the models parameters, $S$ the set of sessions, $\mathbf{M}$ the model and its parameters, $r$ is the rank of a particular document in a session and $c_r^{(s)}$ a indicator function that is $1$ when the document at rank $r$ in session $s$ was clicked and $0$ otherwise.
\begin{align}
	\mathcal{L}\mathcal{L}(S|\mathbf{M}) = \frac{1}{|S|}\sum_{s \in S} \frac{1}{|s|} \sum_{r = 1}^{|s|} \log P(C_r=c_r^{(s)}|M, C_{r-1}, C_{r-2}\dots C_1)
	\label{eq:loglikelihood}
\end{align}


\subsection{Perplexity}
Click perplexity is a widely used metric for evaluating click model accuracy. It measures how surprised a model is to see $c_r^{(s)}$ under the current parameters. Perplexity is calculated for every rank individually, as to see whether some models perform better on documents higher on the SERP page then documents ranked lower on the SERP. It is used as a evaluation metric in \cite{Zhang2011} and \cite{Dupret2008}. The calculation of perplexity can be seen in Equation~\ref{eq:perplexity}.
\begin{align}
	Perplexity_r &= 2^{-\frac{1}{|S|} \sum_{s \in S}(c_r^{(s)} \log_2 p_r + (1-c_r^{(s)} ) \log_2 (1-p_r))} \label{eq:perplexity} \\
	p_r &= P(C_r = 1 | \mathbf{M}) \nonumber
\end{align}
The perplexity of a model is defined as the average of perplexities over all positions. Thus, a smaller perplexity value indicates a better consistency between the click model and the actual click data.

\subsection{Click-Trough-Rate Prediction (CTR)}
The purpose of click-through rates is to measure the ratio of clicks to impressions of an document.
Generally the higher the CTR the higher chance of that document being clicked.
The click-through rate of a document $d$ is defined as:
\begin{align}
	CTR_d = \frac{1}{|S_d|} \sum_{s_d} c_{r_d}^{(s_d)}
\end{align}
where $S_d$ is the set of sessions where document $d$ appears.
A way to use this as an evaluation measure is proposed in \cite[p. 4]{Chapelle2009}. In the same way we calculate the CTR prediction using the following protocol:
\begin{enumerate}
	\item Retrieve all sessions related to a given query.
	\item Consider an url that appears both in position 1 and some other positions.
	\item Hold out as test sessions all the sessions in which that url appeared in position 1.
	\item Train the model on the remaining sessions and predict the relevance.
	\item Compute the test CTR in position 1 on the held-out sessions.
	\item Compute an error between these two quantities.
	\item Average the error on all such urls and queries, weighted by the number of test sessions.
\end{enumerate}

The error measure we use is the Root-Mean-Square-Error (RMSE).

\subsection{Relevance Prediction}
Relevance prediction was used to evaluate performance of the DBN model \cite[p. 6]{Chapelle2009}.
The accuracy of CTR prediction may not directly translate to relevance, especially when we were to evaluate the whole task instead of a single query.
In this case, the CTR of a particular document is highly dependent on the user-model assumption.
For example if a user tends to ignore a document that isn't fresh, CTR will be low even if the document is relevant.
To measure relevance prediction we use a hand annotated set of relevances. We then use the Area Under the Curve (AUC) between the annotated relevances and the predicted relevances as an evaluation measure. We also measure the Pearson correlation between the two. 

\subsection{Predicted Relevance as a Ranking Feature}
In this set of experiments we use the predicted relevance directly to rank urls, we use the model as a ranker. To evaluate the performance of a ranker we use the Normalized Discounted Cumulative Gain (NDCG) \cite{NDCG}, for which we use a cutoff at five (NDCG@5). To calculate the NDCG@5 we only consider the documents for which we have annotated relevances. All these queries are then averaged to calculate the ranking performance of the click model. The algorithm can be seen below:

\begin{enumerate}
	\item Retrieve all session that appear more than 10 times.
	\item Filter out the sessions that don't appear in the editorial judgements.
	\item Train the model on the sessions and predict relevance for the sessions.
	\item Sort the urls w.r.t the predicted relevance given by the model.
	\item Compute the NDCG@5.
	\item Average over all sessions.
\end{enumerate}

\subsection{Computation Time}
Historically in machine learning a big problem in creating accurate models was the amount of data that was available. However this is no longer the case, we are mostly restricted by the time that it takes to learn a model from the large amount of data that we currently have. So an important feature of a succesful click model is that it should be able to efficiently compute its parameters. Therefore, we also decided to look at the computation time it takes to train the click models.

\fixme{TODO: Add some info about machine it is run on???}

\section{Experiments}
In this section we report on the experimental evaluation and comparison of the click models in Section~\ref{sec:methodology} on the evaluation metrics in Section~\ref{sec:evaluation} evaluated on the first 1 million query sessions from the Yandex Relevance Prediction contest of 2011 \ref{YandexData}. The rest of this section will elaborate on the experiments we have done and the results we found. We will also show how query frequency improves the performance of the models. Moreover, we will report on the evaluation measures in combination with click entropy.

\subsection{Experimental Setup}
The Yandex data set was used in the Yandex Relevance Prediction contest of 2011. In the experiments we have done we have used the first 1 million queries. We deliberatly have chosen to keep sessions that are without any clicks because removing them might hurt the performance of certain models, i.e. TCM because the freshness of a document can change with these sessions. These first 1 million sessions contain \fixme{X} distinct queries. These queries get divided in a set of training sessions used to train the click models and a set of test sessions used in the evaluation of the models, the number of sessions in these sets have a 3 to 1 ratio. The problem with this approach in combination with the TCM model is that this removes the guarantee that the complete task is used when training or evaluating the model. We expect that this will hurt the performance of the TCM model and that the actual performance might be higher. 

To report on the performance with regard to query frequency we have split the data into 4 parts. The distribution of these parts can be seen in Table~\ref{table:query_split} \fixme{Add table on query splitting}. In the rest of this section with every evaluation measure we will also report on how this was influenced by the query frequency. The second factor that might influence performance that we have analyzed is the click entropy. The click entropy was also used to analyse queries in \cite{dou2008}. The formal definition is shown in Equation~\ref{eq:entropy}.
\begin{align}
	ClickEntropy(q) &= \sum_{d \in \mathcal{P}(q)} - P(d|q) \log_2 P(d|q) \label{eq:entropy} \\
	P(d|q) &= \frac{\sum_p c_{r_d}^{(q)}}{\sum_{u \in \mathcal{P}(q)} c_{r_u}^{(q)}} \label{eq:entropy_2}
\end{align}

Here $ClickEntropy(q)$ is the click entropy for query $q$. $\mathcal{P}(q)$ are documents clicked on when regarding query $q$. $P(d|q)$ is the percentage of clicks on document $d$ among all clicks on $q$. The search sessions have been divided into 3 parts with respect to click entropy and the evaluation measures have been tested on these parts. Statistics of these parts can be seen in Table~\ref{table:entropy_split}.

\subsection{Results on Loglikelihood}
In Figure \ref{fig:loglikelihood} the results of the loglikelihood experiments can be seen.

\subsection{Results on Perplexity}

\subsection{Results on CTR-Prediction}

\subsection{Results on Relevance Prediction}

\subsection{Results Predicted Relevance as a Ranking Feature}

\subsection{Results on Computation Time}

In Table~\ref{table:results} one can see the results of the experiments.

\begin{table}[h]
	\begin{tabular}{@{}lllllll@{}}
		\toprule
		Model     & Log-likelihood  & Perplexity	 & Rel. Pred. AUC		 & Ranking NDCG      & CTR Pred.		 & Training time (sec) \\ \midrule
		UBM       & \textbf{-0.288} & 1.382          & 0.699                 & 0.875			 & 0.218			 & 10387.7                 \\
		TCM       & -0.307          & \textbf{1.374} & 0.532                 & \textbf{0.977}	 & 0.234			 & 8145.77                 \\
		SimpleDCM & -0.450          & 1.387          & 0.674                 & 0.837             & \textbf{0.210}	 & 186.915                 \\
		SimpleDBN & -0.448          & 1.386          & 0.668                 & 0.740             & \textbf{0.210}	 & \textbf{161.067}        \\
		DCM       & -0.453          & 1.380          & \textbf{0.706}        & 0.825			 & 0.228			 & 18332.5                 \\
		DBN       & -0.375          & 1.383          & 0.436                 & 0.681			 & 0.230			 & 13204.2                 \\ \bottomrule
	\end{tabular}
	\caption{Results}
	\label{table:results}
\end{table}
