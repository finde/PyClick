\section{Evaluation}
\label{sec:evaluation}
%Evaluation:
%	Purpose of the evaluation
%	Data used in the evaluation
%	Evaluation setup
%	Results (present as many results as necessary to illustrate the work you have done on the project - tables are a good way and required by the assignment; plots can be another good way)

% purpose of evaluation
\subsection{Evaluation Criteria}
To equally evaluate each click model's performance, we use evaluation metrics that are proposed along with the click models. The evaluation metrics used in this experiment are listed as follow:

\subsubsection{Likelihood and Perplexity}
Likelihood is already implied in each click model during E-step, where it should get better for every EM iteration. 
However, this evaluation can become interesting when we compare one click model to the others. 
By comparing the likelihood evaluation per iteration, we can see the performance of each click-model based on its convergent speed. \fixme{We are not doing this? We are just calculating the likelihood at the end.}

Click perplexity is a widely used metric for evaluation click model accuracy.
Perplexity can be seen as the log-likelihood powers which are computed independently at each position as also mentioned at \cite{Zhang2011} and \cite{Dupret2008}. 
For example, we assume that $q_j^s$ is the probability of some click calculated from a click model, i.e. $P(C_j^s=1)$ where $C_j^s$ is a binary value indicating the click event at position $j$ in query session $s$. Then the click perplexity at position $j$ is computed as follows:
\begin{align*}
	p_j 
	&= 2^{-\frac{1}{|S|} \sum_{s \in S}(C_j \log_2 q_j + (1-C_j) \log_2 (1-q_j))}
\end{align*}
The perplexity of a data set is defined as the average of perplexities in all positions.
Thus, a smaller perplexity value indicates a better consistency between the click model and the actual click data.

\subsubsection{Click-Trough-Rate prediction (CTR)}
The purpose of click-through rates is to measure the ratio of clicks to impressions of an document.
Generally the higher the CTR the higher change of that document being clicked.
The click-through rate is defined as:
\begin{align*}
	CTR = \frac{\# \, of \, clicks}{\# \, of \, impression}
\end{align*}
A way to use this as an evaluation measure is proposed in \cite[p. 4]{Chapelle2009}. In the same way we calculate the CTR prediction using the following protocol:
\begin{enumerate}
	\item Retrieve all the sessions related to a given query
	\item Consider an url that appeared both in position 1 and some other positions
	\item Hold out as test sessions all the sessions in which that url appeared in position 1
	\item Train the model on the remaining sessions and predict $a_u$
	\item Compute the test CTR in position 1 on the held-out sessions
	\item Compute an error between these two quantities
	\item  Average the error on all such urls and queries, weighted by the number of test sessions
\end{enumerate}

The error measure we use is the Mean-Square-Error (MSE).

\subsubsection{Relevance prediction}
Relevance prediction was used to evaluate performance of DBN model \cite[p. 6]{Chapelle2009}.
The accuracy of CTR prediction may not directly translate to relevance, especially when we were to evaluate TCM.
On TCM, the CTR of a particular document can be very low even if the document is relevant.
This can happen because on TCM, we look over the whole task instead of single query, and the second time this document reappear, the user tends to ignore it by the freshness assumption. \fixme{I don't think we want to mention the performance of the TCM here already.?}
Therefore, relevance prediction can be used as additional inference of relevancy.

\fixme{TODO:: add more info about general relevance algorithm}

\subsubsection{Predicted relevance as a ranking feature}
The accuracy of CTR prediction may not directly translate to relevance. However we can use these relevances by focusing on the relative order of the relevances. For every query for which we have editorial judgements we use the predicted relevances to rank the documents. To evaluate the performance of a ranker we can use the Normalized Discounted Cumulative Gain (NDCG) \cite{NDCG}, for which we use a cutoff at five (NDCG$_5$). All these queries are then averaged to calculate the ranking performance of the click model.

\subsubsection{Computation time}
Historically in Machine Learning a big problem in creating accurate models was the amount of data that was available. However this is no longer the case, we are mostly restricted by the time that it takes to learn a model from the large amount of data that we currently have. So an important feature of a succesful click model is that it should be able to efficiently compute its parameters. Therefor we also decided to look at the computation time it takes to train the click models.

\subsection{Evaluation setup}
The experiment was run ...

\subsection{Results}
In Table~\ref{table:results} one can see the results of the experiments.
It can be seen that ...

\begin{table}[ht]
\centering
\begin{tabular}{l|lll|}
\cline{2-4}
                                          & Log-likelihood  & Perplexity        & Computation Time \\
\cline{1-4}
\multicolumn{1}{|l|}{Click Model A}       & 0               & 0                 &   \\
\multicolumn{1}{|l|}{Click Model B}       & 0               & 0                 &   \\
\cline{1-4}
\end{tabular}
\caption{Results}
\label{table:results}
\end{table}
